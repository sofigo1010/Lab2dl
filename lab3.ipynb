{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bac6c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:03<00:00, 2.77MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 86.1kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 1.74MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.05MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Configuración: MLP_ReLU_1hidden ===\n",
      "Epoca 1/5, Loss: 0.0473\n",
      "Epoca 2/5, Loss: 0.0824\n",
      "Epoca 3/5, Loss: 0.0689\n",
      "Epoca 4/5, Loss: 0.1716\n",
      "Epoca 5/5, Loss: 0.0095\n",
      "Accuracy en test: 0.9739\n",
      "\n",
      "=== Configuración: MLP_Tanh_2hidden ===\n",
      "Epoca 1/7, Loss: 0.2780\n",
      "Epoca 2/7, Loss: 0.0926\n",
      "Epoca 3/7, Loss: 0.1869\n",
      "Epoca 4/7, Loss: 0.0258\n",
      "Epoca 5/7, Loss: 0.1125\n",
      "Epoca 6/7, Loss: 0.0327\n",
      "Epoca 7/7, Loss: 0.0385\n",
      "Accuracy en test: 0.9815\n",
      "\n",
      "=== Configuración: MLP_Sigmoid_3hidden ===\n",
      "Epoca 1/10, Loss: 1.7137\n",
      "Epoca 2/10, Loss: 1.1876\n",
      "Epoca 3/10, Loss: 0.6652\n",
      "Epoca 4/10, Loss: 0.5871\n",
      "Epoca 5/10, Loss: 0.3836\n",
      "Epoca 6/10, Loss: 0.3506\n",
      "Epoca 7/10, Loss: 0.2259\n",
      "Epoca 8/10, Loss: 0.2991\n",
      "Epoca 9/10, Loss: 0.1763\n",
      "Epoca 10/10, Loss: 0.1871\n",
      "Accuracy en test: 0.9426\n",
      "\n",
      "Probando: layers=[256], act=relu, lr=0.001, batch_size=64\n",
      "Probando: layers=[256], act=relu, lr=0.001, batch_size=128\n",
      "Probando: layers=[256], act=relu, lr=0.0005, batch_size=64\n",
      "Probando: layers=[256], act=relu, lr=0.0005, batch_size=128\n",
      "Probando: layers=[256], act=tanh, lr=0.001, batch_size=64\n",
      "Probando: layers=[256], act=tanh, lr=0.001, batch_size=128\n",
      "Probando: layers=[256], act=tanh, lr=0.0005, batch_size=64\n",
      "Probando: layers=[256], act=tanh, lr=0.0005, batch_size=128\n",
      "Probando: layers=[256, 128], act=relu, lr=0.001, batch_size=64\n",
      "Probando: layers=[256, 128], act=relu, lr=0.001, batch_size=128\n",
      "Probando: layers=[256, 128], act=relu, lr=0.0005, batch_size=64\n",
      "Probando: layers=[256, 128], act=relu, lr=0.0005, batch_size=128\n",
      "Probando: layers=[256, 128], act=tanh, lr=0.001, batch_size=64\n",
      "Probando: layers=[256, 128], act=tanh, lr=0.001, batch_size=128\n",
      "Probando: layers=[256, 128], act=tanh, lr=0.0005, batch_size=64\n",
      "Probando: layers=[256, 128], act=tanh, lr=0.0005, batch_size=128\n",
      "\n",
      "Mejor configuración grid search: {'config': {'name': 'GS', 'hidden_layers': [256], 'activation': 'relu', 'lr': 0.0005, 'batch_size': 64, 'epochs': 5}, 'accuracy': 0.9811}\n",
      "Ranking de modelos:\n",
      "                  Model  Accuracy\n",
      "1     MLP_Tanh_2hidden    0.9815\n",
      "3       GridSearchBest    0.9811\n",
      "0     MLP_ReLU_1hidden    0.9739\n",
      "2  MLP_Sigmoid_3hidden    0.9426\n",
      "\n",
      "-- ¿Qué hiperparámetros influyeron más? --\n",
      "Generalmente el número de neuronas y la elección de la función de activación tienen gran impacto,\n",
      "así como la tasa de aprendizaje. Batch size y epochs afectan estabilidad y convergencia.\n"
     ]
    }
   ],
   "source": [
    "# ## 1. Carga del Dataset\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "# Transformaciones: Normalización y conversión a tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # mean y std de MNIST\n",
    "])\n",
    "\n",
    "# Descarga y creación de dataloaders\n",
    "train_dataset = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.MNIST(root='data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Para experimentación, los batch_size variarán por configuración\n",
    "# DataLoader de ejemplo (se redefinirá según configuración)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "# ## 2. Definición de la MLP flexible\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, output_size, activation_fn):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_features = input_size\n",
    "        # Capas ocultas\n",
    "        for h in hidden_layers:\n",
    "            layers.append(nn.Linear(in_features, h))\n",
    "            layers.append(activation_fn())\n",
    "            in_features = h\n",
    "        # Capa de salida\n",
    "        layers.append(nn.Linear(in_features, output_size))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # aplanar imágenes 28x28\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def train_and_evaluate(config, verbose=True):\n",
    "    \"\"\"\n",
    "    Entrena y evalúa un modelo MLP según la configuración dada.\n",
    "    config: dict con keys:\n",
    "      'hidden_layers', 'activation', 'lr', 'batch_size', 'epochs'\n",
    "    Devuelve: accuracy en test\n",
    "    \"\"\"\n",
    "    # Selección de función de activación\n",
    "    activations = {\n",
    "        'relu': nn.ReLU,\n",
    "        'tanh': nn.Tanh,\n",
    "        'sigmoid': nn.Sigmoid,\n",
    "    }\n",
    "    act_fn = activations[config['activation']]\n",
    "\n",
    "    # Dataloaders según batch_size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    test_loader  = DataLoader(test_dataset,  batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "    # Modelo, optimizador y criterio\n",
    "    model = MLP(784, config['hidden_layers'], 10, act_fn)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Loop de entrenamiento\n",
    "    for epoch in range(1, config['epochs'] + 1):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(X_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if verbose:\n",
    "            print(f\"Epoca {epoch}/{config['epochs']}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Evaluación\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            logits = model(X_batch)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "    accuracy = correct / total\n",
    "    if verbose:\n",
    "        print(f\"Accuracy en test: {accuracy:.4f}\\n\")\n",
    "    return accuracy\n",
    "\n",
    "# ## 3. Configuraciones iniciales\n",
    "configs = [\n",
    "    {\n",
    "        'name': 'MLP_ReLU_1hidden',\n",
    "        'hidden_layers': [128],\n",
    "        'activation': 'relu',\n",
    "        'lr': 1e-3,\n",
    "        'batch_size': 64,\n",
    "        'epochs': 5\n",
    "    },\n",
    "    {\n",
    "        'name': 'MLP_Tanh_2hidden',\n",
    "        'hidden_layers': [256, 128],\n",
    "        'activation': 'tanh',\n",
    "        'lr': 5e-4,\n",
    "        'batch_size': 128,\n",
    "        'epochs': 7\n",
    "    },\n",
    "    {\n",
    "        'name': 'MLP_Sigmoid_3hidden',\n",
    "        'hidden_layers': [512, 256, 128],\n",
    "        'activation': 'sigmoid',\n",
    "        'lr': 1e-4,\n",
    "        'batch_size': 256,\n",
    "        'epochs': 10\n",
    "    }\n",
    "]\n",
    "\n",
    "# Ejecutar entrenamientos\n",
    "results = []\n",
    "for cfg in configs:\n",
    "    print(f\"=== Configuración: {cfg['name']} ===\")\n",
    "    acc = train_and_evaluate(cfg)\n",
    "    results.append({'Model': cfg['name'], 'Accuracy': acc})\n",
    "\n",
    "# ## 4. Tuning de Hiperparámetros (Grid Search)\n",
    "from itertools import product\n",
    "\n",
    "grid = {\n",
    "    'hidden_layers': [[256], [256, 128]],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'lr': [1e-3, 5e-4],\n",
    "    'batch_size': [64, 128],\n",
    "    'epochs': [5]\n",
    "}\n",
    "\n",
    "best = {'config': None, 'accuracy': 0}\n",
    "for hl, act, lr, bs, ep in product(grid['hidden_layers'], grid['activation'], grid['lr'], grid['batch_size'], grid['epochs']):\n",
    "    cfg = {'name': 'GS', 'hidden_layers': hl, 'activation': act, 'lr': lr, 'batch_size': bs, 'epochs': ep}\n",
    "    print(f\"Probando: layers={hl}, act={act}, lr={lr}, batch_size={bs}\")\n",
    "    acc = train_and_evaluate(cfg, verbose=False)\n",
    "    if acc > best['accuracy']:\n",
    "        best = {'config': cfg, 'accuracy': acc}\n",
    "\n",
    "print(\"\\nMejor configuración grid search:\", best)\n",
    "\n",
    "# ## 5. Evaluación comparativa y reporte\n",
    "results.append({'Model': 'GridSearchBest', 'Accuracy': best['accuracy']})\n",
    "\n",
    "df_results = pd.DataFrame(results).sort_values(by='Accuracy', ascending=False)\n",
    "print(\"Ranking de modelos:\\n\", df_results)\n",
    "\n",
    "# Conclusiones (imprimir manualmente al finalizar)\n",
    "print(\"\\n-- ¿Qué hiperparámetros influyeron más? --\")\n",
    "print(\"Generalmente el número de neuronas y la elección de la función de activación tienen gran impacto,\")\n",
    "print(\"así como la tasa de aprendizaje. Batch size y epochs afectan estabilidad y convergencia.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab3685b",
   "metadata": {},
   "source": [
    "# Comparación de resultados\n",
    "\n",
    "La red con **Tanh y dos capas ocultas (256 → 128)** obtuvo el mejor accuracy (98.15 %), seguida muy de cerca por la mejor configuración del Grid Search (ReLU, [256], lr = 5e-4, bs = 64) con 98.11 %. La MLP con ReLU y una sola capa (128) quedó en 97.39 %, y la de tres capas con Sigmoid en 94.26 %.\n",
    "\n",
    "| Posición | Modelo                  | Accuracy |\n",
    "|:--------:|:-----------------------:|:--------:|\n",
    "| 1        | MLP_Tanh_2hidden        | 98.15 %  |\n",
    "| 2        | GridSearchBest          | 98.11 %  |\n",
    "| 3        | MLP_ReLU_1hidden        | 97.39 %  |\n",
    "| 4        | MLP_Sigmoid_3hidden     | 94.26 %  |\n",
    "\n",
    "---\n",
    "\n",
    "## ¿Qué hiperparámetros influyeron más en la mejora del rendimiento? ¿Por qué?\n",
    "\n",
    "- **Función de activación**  \n",
    "  - **Tanh** superó a ReLU gracias a centrar sus salidas en cero, mejorando la dinámica del gradiente.  \n",
    "  - **Sigmoid** presentó saturación y gradientes muy pequeños en redes profundas, enlenteciendo la convergencia.\n",
    "\n",
    "- **Arquitectura (número y tamaño de capas ocultas)**  \n",
    "  - Dos capas con suficiente ancho (256 → 128) modelaron mejor las complejidades del MNIST sin incurrir en los problemas de entrenamiento de redes muy profundas.\n",
    "\n",
    "- **Tasa de aprendizaje (learning rate)**  \n",
    "  - Un **lr = 5e-4** logró un balance óptimo entre velocidad de convergencia y estabilidad, evitando oscilaciones.\n",
    "\n",
    "- **Batch size**  \n",
    "  - Un **batch size pequeño (64)** introdujo ruido en el gradiente que actuó como regularizador, mejorando la generalización frente a bs = 128.\n",
    "\n",
    "En conjunto, la **elección de la activación** y la **arquitectura de la red** fueron los factores clave, seguidos por la **tasa de aprendizaje** y el **tamaño de lote**, ya que determinan cómo y con qué rapidez el modelo explora el espacio de parámetros durante el entrenamiento.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
